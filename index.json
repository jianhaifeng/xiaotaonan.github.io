[{"content":"1.Is generational ZGC fully implemented in version 21 of openjdk? Or is it partially realized? Fully Implemented\n2.What is the main difference between generational ZGC and ordinary ZGC? Ordinary ZGC: collect all objects every time Generational ZGC: Recycling by generation We believe that Generational ZGC will be better suited for most use cases than its predecessor. Some workloads may even see throughput improvements with generational ZGC due to lower resource usage. For example, when running the Apache Cassandra benchmark, generational ZGC required a quarter the heap size but achieved four times the throughput compared to non-generational ZGC, while still keeping pause times under one millisecond. .\nSome workloads are non-generational in nature and may experience slight performance degradation. We believe this is a small enough set of workloads to justify the cost of maintaining two separate versions of ZGC over the long term.\nAnother source of overhead is the more powerful GC barrier. We expect much of this to be offset by the benefit of not having to collect old generation objects as frequently.\nAnother source of additional overhead is running two garbage collectors simultaneously. We need to make sure to balance their call rate and CPU consumption so that they don\u0026rsquo;t impact the application unduly.\nAs is normal with GC development, future improvements and optimizations will be driven by benchmarking and user feedback. We intend to continue improving Generational ZGC even after the first version is released.\n3.During the development process of OpenJDK by generation ZGC, their development time span, R\u0026amp;D personnel and other information. The main purpose is to have an estimate of the approximate effort of this work. 3.1.JDK21-Schedule Schedule 2023/06/08\tRampdown Phase One (fork from main line-JDK17-LTS) 2023/07/20\tRampdown Phase Two 2023/08/10\tInitial Release Candidate 2023/08/24\tFinal Release Candidate 2023/09/19\tGeneral Availability Amost 103days\n3.2.Generational ZGC-R\u0026amp;D staff Co-authored-by: Stefan Karlsson Co-authored-by: Erik Österlund Co-authored-by: Axel Boldt-Christmas Co-authored-by: Per Liden Co-authored-by: Stefan Johansson Co-authored-by: Albert Mingkun Yang Co-authored-by: Erik Helin Co-authored-by: Roberto Castañeda Lozano Co-authored-by: Nils Eliasson Co-authored-by: Martin Doerr Co-authored-by: Leslie Zhai Co-authored-by: Fei Yang Co-authored-by: Yadong Wang Reviewed-by: eosterlund, aboldtch, rcastanedalo 13人\n3.3.Generational ZGC-workload 3.3.1.Functional scope JEPS439\nNo multi-mapped memory\nOptimized barriers\n2. 1.Fast paths and slow paths\n2. 2.Minimizing load barrier responsibilities\n2. 3.Double-buffered remembered sets\n2. 4.Remembered-set barriers\n2. 5.SATB marking barriers\n2. 6.Fused store barrier checks\n2. 7.Store barrier buffers\n2. 8.Barrier patching\nRelocations without additional heap memory\nDense heap regions\nLarge objects\nFull garbage collections\n3.3.2.Amount of code Showing 667 changed files with 63,137 additions and 7,698 deletions.\n4.Is it possible to backport this function to java17? 4.1.Throughput In terms of throughput, generational ZGC improves by about 10% over single-generation ZGC in JDK 17 and a little over 10% over single-generation ZGC in JDK 21, which dropped slightly.\n4.2.Latency The average latency of generational ZGC decreases slightly compared to single-generation ZGC.\nZGC starts to perform well when maximum pause times are considered. The graph below shows a 10-20% improvement in P99 pause time.\nThe biggest advantage of generational ZGC is that it significantly reduces the biggest problem of single-generation ZGC - the possibility of allocation stagnation. Allocation stagnation means that new objects are allocated faster than the ZGC can reclaim memory.\n5.Verification scenario 5.1.Multi-client concurrency This problem can be seen if we switch the use case to Apache Cassandra and look at 99.999%. The graph below shows that up to 75 concurrent clients, single-generation ZGC and multi-generation ZGC have similar performance. However, with more than 75 concurrent clients, the single-generation ZGC becomes overwhelmed and encounters allocation stagnation issues. Generational ZGC, on the other hand, did not encounter this situation and maintained consistent pause times even with up to 275 concurrent clients.\n5.2.P99.99 Event Requires that 99.99% of requests should be faster than the given latency. In other words, only 0.01% of requests are allowed to be slowed down. Much like the old run, single-generation ZGC performed very well under low load, but as allocation pressure increased, worse latencies increased. With Generational ZGC, this is no longer the case. p99.99 latency is very low even under heavy load.\n","permalink":"https://xiaotaonan.github.io/posts/generational-zgc/","summary":"1.Is generational ZGC fully implemented in version 21 of openjdk? Or is it partially realized? Fully Implemented\n2.What is the main difference between generational ZGC and ordinary ZGC? Ordinary ZGC: collect all objects every time Generational ZGC: Recycling by generation We believe that Generational ZGC will be better suited for most use cases than its predecessor. Some workloads may even see throughput improvements with generational ZGC due to lower resource usage.","title":"Generational ZGC"},{"content":"1.Comparison Test 1.1.Test Environment Notebook（3.1 GHz Intel Core i5, 4Gb RAM and 50Gb SSD.）\nOperating System Kernel（3.10.0-1160.102.1.el7.x86_64）\nOperating System（CentOS:7.9 VM）\nPlatform: x86_64\nVM Parameters：No other parameters are used except the parameters for storing and loading mirror logs.\n2.Testing Scenarios 2.1.Scenarios 1：Time Of First Operation 2.1.1.Question 1：Why does Azul Prime ReadyNow experience a decrease in acceleration compared to CRaC when it is launched for the second time? 2.2.Scenarios 2：Time To Complete 100000 Operations 2.2.1.Question 2：Why is Azul Prime ReadyNow better than CRaC in terms of latency reduction for 100,000 requests? 3.Supplementary Material 3.1.CRaC is a Azul-led contribution to the OpenJDK community. In common sense, Azul Prime ReadyNow as a commercial paid version of Azul should be better than the open source CRaC.\n3.1.1.Question 3：Is the Azul Prime ReadyNow installation package correct? 3.1.2.Question 4：Are the use cases used for verification correct? 4.Problem Analysis 4.1.Question 1：Why does Azul Prime ReadyNow experience a decrease in acceleration compared to CRaC when it is launched for the second time? 4.1.1.Guess: caused by serial/parallel file processing 4.1.1.1.File structure used for recovery by CRaC - multi-file composition 4.1.1.1.1.Screenshot of first startup - startup completed in 2.762 seconds 4.1.1.1.2.Second startup screenshot - process level recovery (process ID is the same) 4.1.1.2.File structure used for recovery by ReadyNow - single file composition 4.1.1.2.1.Screenshot of first startup - startup completed in 4.678 seconds 4.1.1.2.2.Second startup screenshot-RootAC (16.2% speed increase), APP (64.9% speed increase) 4.1.1.3.Summarize 1.From the perspective of file loading efficiency alone, parallel file loading should be more efficient than serial file processing.\n2.When learning from ReadyNow\u0026rsquo;s solution, the files used for recovery can be divided into multiple files for processing. Verify whether the second startup time can be further optimized? [a. Next steps to consider]\n4.2.Question 2：Why is Azul Prime ReadyNow better than CRaC in terms of latency reduction for 100,000 requests? 4.2.1.ReadyNow has JVM optimization-Falcon JIT Compiler, while CRaC does not optimize JVM per se 4.2.1.1.CRaC-Hotspot VM CRaC itself does not optimize the JVM.\n4.2.1.2.ReadyNow-Zing VM ReadyNow replaces C2 with Azul’s self-developed Falcon JIT Compiler.\n4.2.2.Summarize 1.What profiles can be directly compiled by Zing VM\u0026rsquo;s Falcon JIT Compiler? [b. Next steps to consider]\n2.You can consider using Graal to replace C2 in Hotspot VM. Will the verification request latency be further reduced? [c. Next steps to consider]\n4.3.Question 3：Is the Azul Prime ReadyNow installation package correct? 4.3.1.Current download address Azul Prime\n4.4.Question 4：Are the use cases used for verification correct? 4.4.1.Currently used test cases-microservice framework spring-boot-3.2.0\nmicronaut-3.8.7\nquarkus-2.16.5.Final\n","permalink":"https://xiaotaonan.github.io/posts/readynow-crac/","summary":"1.Comparison Test 1.1.Test Environment Notebook（3.1 GHz Intel Core i5, 4Gb RAM and 50Gb SSD.）\nOperating System Kernel（3.10.0-1160.102.1.el7.x86_64）\nOperating System（CentOS:7.9 VM）\nPlatform: x86_64\nVM Parameters：No other parameters are used except the parameters for storing and loading mirror logs.\n2.Testing Scenarios 2.1.Scenarios 1：Time Of First Operation 2.1.1.Question 1：Why does Azul Prime ReadyNow experience a decrease in acceleration compared to CRaC when it is launched for the second time? 2.2.Scenarios 2：Time To Complete 100000 Operations 2.","title":"Comparison of ReadyNow and CRaC"},{"content":"1.Scenarios View 2.Logical View Note: 29 classes, does not include C source code\n3.Development View 4.Handle View 4.1.Start Application 4.2.Test and Tune 4.3.Checkpoint Generation Note: By default, CRaC will automatically stop running Java applications when generating checkpoint images. However, Azul Prime ReadyNow, Spring-boot, etc. support generating checkpoint images according to other rules such as cycles without stopping running Java applications. This function Can also be used in production environments.\n4.4.Restore 4.Physical View slightly.\n","permalink":"https://xiaotaonan.github.io/posts/crac41/","summary":"1.Scenarios View 2.Logical View Note: 29 classes, does not include C source code\n3.Development View 4.Handle View 4.1.Start Application 4.2.Test and Tune 4.3.Checkpoint Generation Note: By default, CRaC will automatically stop running Java applications when generating checkpoint images. However, Azul Prime ReadyNow, Spring-boot, etc. support generating checkpoint images according to other rules such as cycles without stopping running Java applications. This function Can also be used in production environments.\n4.4.Restore 4.","title":"CRaC: Brief analysis of 4+1 view architecture"},{"content":"0.Preface 0.1.Java Program Execution Process 0.2.Compilation Is Fast Or Slow？ 0.2.1.Java Virtual Machine (Interpreted is relatively slow) 0.2.2.Just In Time Compiler (relatively fast) 0.2.3.Java Compiler (relatively fast) Eg: Graal compiler (can be used as an AOT compiler and can replace C2 in the JIT compiler), AOT (jaotc)\n1.JIT Compilation Process After the Java application is started, after class loading and bytecode verification are completed, the JIT compiler will not be triggered immediately for compilation, but will be interpreted and analyzed by the real-time interpreter first; After the just-in-time interpreter completes the preliminary interpretation and analysis, the JIT compiler will use the analysis information that has been collected to find hot spots (frequently executed code parts). Once the hot spot codes are available, C1 can Conduct analysis and compile and generate relatively efficient target machine code based on the analysis results, so that the Java virtual machine at this time has the code performance of the native machine. At the same time, C1 will also perform further analysis; After C1 completes further analysis, C2 will use the analysis information generated by C1 to perform more aggressive and time-consuming optimization. At this time, C2 will recompile the code to generate more efficient target machine code, thereby making it more efficient. Significantly improve the code performance of Java virtual machine. In summary, based on more information about hotspots, C1 performance improves faster, while C2 performance improves better. 2.Current Needs Azul can preheat in the simulation environment (simulate hotspot methods and loop bodies), then inject the results into the production environment, and compile directly using C2 to reduce the compilation time at runtime. This is very effective for scenarios such as securities industry quotations, because these scenarios require high-speed operation from the beginning.\nHorizontal axis: the time when the JVM virtual machine reaches the best code performance; Vertical axis: JVM’s best code performance degree or ratio; Glitches: caused by de-optimization or garbage collection. 3.Question Azul can learn and train in a simulation environment, and then input the training result set as a reference into the production environment to achieve the peak effect at startup. And eliminate GC glitches.\n3.1.Current Issues Long startup time; It takes a long time for the Java Virtual Machine to reach the optimal code performance of the Java Virtual Machine. 3.2.Desired Results Short startup time; After startup, the Java virtual machine can quickly achieve optimal code performance. 3.3.Desired Goals On the premise of ensuring the correct use of key functions, significantly reduce the time it takes to restart; Eliminate glitches and enable the Java virtual machine to quickly achieve optimal code performance. 4.Problem Analysis 4.1.What are the current plans to accelerate the start-up? 4.1.1.CDS (Class Data Sharing) Functional positioning: Dump internal classes, application classes, dynamic (classes loaded by custom class loader and omit the dump classlist step) and other representations into files (class loader, jsa file); Shared (CDS) on each JVM startup. Insufficient: No optimization or hotspot detection; Only class loading time is reduced; The startup speed is not significantly accelerated.\n4.1.2.AOT (Ahead Of Time Compilation, compilation in advance, source code to machine code) Advantage: \u0026ldquo;Full speed\u0026rdquo; from the start, GraalVM native images can do this; By definition, AOT is static, code is compiled before running, and there is no overhead in compiling code at runtime; Small memory footprint\nInsufficient: Does not interpret bytecode; There is no hotspot analysis; There is no runtime compilation of code, so no runtime bytecode generation; Limited use of method inlining; Reflection is possible but complicated; Cannot use speculative optimization (assuming conditions hold, such as array bounds) Must be compiled for common characteristics (e.g. same name, same parameters, etc.) Because optimization is not thorough, overall performance is usually lower; Deploy the environment! = development environment.\n4.1.3.JIT (Just In Time Compilation, instant compilation) advantage: Aggressive method inlining can be used at runtime (for example, the number of method lines does not exceed 80 lines, aggressive refers to large methods, long compilation time, and large binary files) Can be generated using runtime bytecode Reflection is simple Can speculative optimization be used? (assuming conditions hold, such as array bounds) It can even be optimized for Haswell, Skylake, Ice Lake, and more. (CPU architecture) Overall performance is usually higher Deployment environment == development environment\ninsufficient: Takes more time to start (but will be faster) There is an overhead in compiling code at runtime Larger memory footprint\n4.2.Why glitches occur (the existence of glitches directly affects the optimal performance of the Java virtual machine)？ 4.2.1.De-optimization Although C2-compiled code is highly optimized and long-lived, it can also be de-optimized. As a result, the JVM will temporarily roll back to the interpreted state. De-optimization occurs when the compiler\u0026rsquo;s optimistic assumptions prove to be wrong - for example, when profile information does not match method behavior, the JVM de-optimizes compiled and inlined code as soon as the hot path changes .\n4.2.2.There are GC operations before reaching optimal performance 5.Solutions 5.1.(Commercial fee)Azul Prime ReadyNow 5.1.1.What is Azul Prime ReadyNow? ReadyNow is a feature of Azul Platform Prime that, when enabled, significantly improves application warm-up behavior.\n5.1.2.What is preheating? Warm-up refers to the time required for a Java application to reach optimal performance. The just-in-time (JIT) compiler\u0026rsquo;s job is to provide optimal performance by generating optimized compiled code from application bytecode. This process takes some time as the JIT compiler looks for optimization opportunities based on analysis of the application.\n5.1.3.The basic idea ReadyNow retains profiling information collected during an application\u0026rsquo;s run so that subsequent runs do not have to learn from scratch again. Warming up improves the operation of each application until peak performance is achieved.\n5.1.4.Instructions To enable ReadyNow, add the following command line options, which are usually the same for both: ● -XX:ProfileLogIn= instructs Azul Platform Prime to use information from the existing profile log. ● -XX:ProfileLogOut= records previous compilation and runtime de-optimization decisions. Running the application will automatically generate or update the profile log. This profile log will be used on subsequent runs of the application, improving warm-up.\n5.1.5.Integrated development Closed source\n5.2.(Open Source Free)CRaC 5.2.1.What is CRaC? Referenced to CRIU (Checkpoint Restore In Userspace, checkpoint recovery in user space) The CRaC (Coordinated Restore at Checkpoint) project studies the coordination of Java programs and mechanisms for checkpointing (making mirrors and snapshots) of Java instances during execution. Restoring from an image can resolve some issues with boot and warm-up times. The main goal of this project is to develop a new mechanism-agnostic standard API to inform Java programs about checkpoint and recovery events. Other research activities will include, but are not limited to, integration with existing checkpoint/recovery mechanisms and the development of new mechanisms, changes to the JVM and JDK to shrink images and ensure they are correct.\n5.2.2.The basic idea Input data (simulated request) to a Java application running in a specific (canary) environment (CPU, memory, I/O, operating system, etc.). When the input data reaches saturation (the requested path covers all use case), freeze the running application, output the checkpoint of the Java virtual machine to achieve optimal performance as a snapshot file and save it. Later, the application can be started through the previously saved image file (theoretically it can be a different physical machine).\n5.2.3.Deployment process Initiate simulated requests to applications running in the canary environment, generate snapshot files when saturated requests are reached, and then restore the application through the snapshot files in the production environment.\n5.2.4.Instructions To enable CRaC, add the following command line options, which are usually the same for both: ● -XX:CRaCRestoreFrom=cr instructs the JDK to use the information in the existing configuration file log. ● -XX:CRaCCheckpointTo=cr records previous compilation and running de-optimization decisions.\n5.2.5.Integrated development 5.2.5.1.Get source code git clone https://github.com/openjdk/crac.git -b {tag} 5.2.5.2.Manual compilation bash configure make images mv build/linux-x86_64-server-release/images/jdk/ . 5.2.5.3.Download the compatible CRIU CRIU\n5.2.5.3.Extract and copy the criu binary on the same named file in the JDK cp criu-dist/sbin/criu jdk/lib/criu 5.2.5.4.Execute authorization sudo chown root:root jdk/lib/criu sudo chmod u+s jdk/lib/criu 6.Verify 6.1.Official verification 6.1.1.Verification environment Laptop (Intel i7-5500U, 16Gb RAM and SSD.) Operating system kernel (Linux kernel 5.7.4-arch1-1) Operating system (ubuntu:18.04 based image) Platform: archlinux\n6.1.2.Verification scenario 6.1.2.1.Scenario 1: Time To First Operation 6.1.2.2.Scenario 2: Time to Complete N operations:sprint-boot(OpenJDK ON/OFF CRaC) 6.1.2.3.Scenario 3: Time to Complete N operations:quarkus(OpenJDK ON/OFF CRaC) 6.1.2.4.Scenario 4: Time to Complete N operations:micronaut(OpenJDK ON/OFF CRaC) 6.2.Local verification 6.2.1.Verification environment Laptop (3.1 GHz Intel Core i5, 4Gb RAM and 50Gb SSD.) Operating system kernel (3.10.0-1160.102.1.el7.x86_64) Operating system (CentOS:7.9 VM) Platform: x86_64\n6.2.2.Verification scenario 6.2.2.1.Scenario 1: Time Of First Operation 6.2.2.2.Scenario 2: Time To Complete 100000 Operations ","permalink":"https://xiaotaonan.github.io/posts/jit-warmstartup/","summary":"0.Preface 0.1.Java Program Execution Process 0.2.Compilation Is Fast Or Slow？ 0.2.1.Java Virtual Machine (Interpreted is relatively slow) 0.2.2.Just In Time Compiler (relatively fast) 0.2.3.Java Compiler (relatively fast) Eg: Graal compiler (can be used as an AOT compiler and can replace C2 in the JIT compiler), AOT (jaotc)\n1.JIT Compilation Process After the Java application is started, after class loading and bytecode verification are completed, the JIT compiler will not be triggered immediately for compilation, but will be interpreted and analyzed by the real-time interpreter first; After the just-in-time interpreter completes the preliminary interpretation and analysis, the JIT compiler will use the analysis information that has been collected to find hot spots (frequently executed code parts).","title":"JIT and Tuning Warm-up"}]